{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please ignore the syntax warnings as small integers in CPython are singletons\n",
      "Using `is` instead of `=` for comparison in performance-critical code is acceptable\n",
      "Downloading stopwords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\10022\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All languages: ['afrikaans', 'arabic', 'breton', 'bulgarian', 'catalan', 'chinese', 'czech', 'danish', 'dutch', 'english', 'esperanto', 'estonian', 'finnish', 'french', 'galician', 'german', 'greek', 'hebrew', 'hungarian', 'icelandic', 'interlingua ', 'inuktitut', 'irish', 'italian', 'japanese', 'korean', 'latin', 'lithuanian', 'maori', 'norwegian', 'occitan ', 'persian', 'polish', 'portuguese', 'romanian', 'russian', 'sanskrit', 'serbian', 'slovenian', 'spanish', 'swedish', 'tagalog', 'telugu', 'tibetan', 'welsh', 'western frisian', 'yiddish']\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "\n",
    "from KeywordSearch.loader import metadata, stemmer, processed_books, LOOKUP_TABLE_PATH\n",
    "from KeywordSearch.kwsearch import regex_tokenise\n",
    "from KeywordSearch.utils import save_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_text(input_str: str) -> list[str]:\n",
    "    return stemmer.stemWords(regex_tokenise.findall(unidecode(input_str).casefold()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_meta_data(metadata_dict: dict):\n",
    "    saving_path = LOOKUP_TABLE_PATH.replace(\"lookup_table.npz\", \"%s\")\n",
    "\n",
    "    subject_vocab, title_vocab, author_vocab = set(), set(), set()\n",
    "    tokens_buffer = dict()\n",
    "    for book_id, (_, subjects, title, author) in metadata_dict.items():\n",
    "        subject_tokens = subjects\n",
    "        title_tokens = tokenise_text(title)\n",
    "        author_tokens = tokenise_text(author)\n",
    "        subject_vocab.update(subject_tokens)\n",
    "        title_vocab.update(title_tokens)\n",
    "        author_vocab.update(author_tokens)\n",
    "        tokens_buffer[book_id] = (subject_tokens, title_tokens, author_tokens)\n",
    "\n",
    "    subject_dict = {sub : i for i, sub in enumerate(sorted(subject_vocab))}\n",
    "    title_dict = {w : i for i, w in enumerate(sorted(title_vocab))}\n",
    "    author_dict = {w : i for i, w in enumerate(sorted(author_vocab))}\n",
    "    book_limit = max(processed_books) + 1\n",
    "    index_sub = scipy.sparse.dok_array((len(subject_dict), book_limit), dtype=np.bool_)\n",
    "    index_ti = scipy.sparse.dok_array((len(title_dict), book_limit), dtype=np.bool_)\n",
    "    index_au = scipy.sparse.dok_array((len(author_dict), book_limit), dtype=np.bool_)\n",
    "    \n",
    "    for book_id, (subject_tokens, title_tokens, author_tokens) in tokens_buffer.items():\n",
    "        for token in subject_tokens:\n",
    "            index_sub[subject_dict[token], book_id] = True\n",
    "        for token in title_tokens:\n",
    "            index_ti[title_dict[token], book_id] = True\n",
    "        for token in author_tokens:\n",
    "            index_au[author_dict[token], book_id] = True\n",
    "\n",
    "    scipy.sparse.save_npz(saving_path %(\"subject_index.npz\"), index_sub.tocsr())\n",
    "    scipy.sparse.save_npz(saving_path %(\"title_index.npz\"), index_ti.tocsr())\n",
    "    scipy.sparse.save_npz(saving_path %(\"author_index.npz\"), index_au.tocsr())\n",
    "\n",
    "    save_pickle((subject_dict, title_dict, author_dict), saving_path %(\"metadata_index_lookup.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_meta_data(metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
