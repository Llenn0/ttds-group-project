{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "backend_url = \"https://ttds-gutenberg-fvyohsgcaq-nw.a.run.app/\"\n",
    "regex_language_line = re.compile(r\"(\\w+) \\([0-9]+ phrases\\):\")\n",
    "regex_phrase_line = re.compile(r\"(.+) \\((.+)\\)\")\n",
    "\n",
    "def fetch_english_testset(dir_: str=\"test_sets\", fname: str=\"generic.txt\"):\n",
    "    with open(os.path.join(dir_, fname), 'r', encoding=\"utf-8\") as f:\n",
    "        queries = f.read().splitlines()\n",
    "    return {q.strip('\"') : [] for q in queries if q}\n",
    "\n",
    "def fetch_multilingual_testset(dir_: str=\"test_sets\", fname: str=\"multi-lingual.txt\"):\n",
    "    all_queries: dict[str, dict] = dict()\n",
    "    current_language = None\n",
    "    with open(os.path.join(dir_, fname), 'r', encoding=\"utf-8\") as f:\n",
    "        queries = f.read().splitlines()\n",
    "    for line in queries:\n",
    "        match_language_line = regex_language_line.match(line)\n",
    "        if match_language_line is not None:\n",
    "            current_language = match_language_line.group(1).strip().casefold()\n",
    "            all_queries[current_language] = dict()\n",
    "            continue\n",
    "        match_phrase_line = regex_phrase_line.match(line)\n",
    "        all_queries[current_language][(match_phrase_line.group(1), match_phrase_line.group(2))] = []\n",
    "    return all_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic = fetch_english_testset(fname=\"generic.txt\")\n",
    "memorable = fetch_english_testset(fname=\"memorable.txt\")\n",
    "multi_lingual = fetch_multilingual_testset(fname=\"multi-lingual.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_template = {\n",
    "    \"query\"     : \"\",\n",
    "    \"languages\" : [\"english\"],\n",
    "    \"subjects\"  : [],\n",
    "    \"page\"      : 1,\n",
    "    \"dist\"      : 1,\n",
    "    \"numPerPage\": 100000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert requests.post(backend_url + \"phrase\", json=query_template).json()[\"err_msg\"] == \"No error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_english(rounds: int, dist: int, *datasets):\n",
    "    with tqdm(total=len(datasets) * 100 * rounds, ncols=80, leave=False) as pbar:\n",
    "        for dataset in datasets:\n",
    "            requests.post(backend_url + \"setcache\", json={\"clear_cache\" : True})\n",
    "            for _ in range(rounds):\n",
    "                for query_str, record in dataset.items():\n",
    "                    query_json = query_template.copy()\n",
    "                    query_json[\"query\"] = query_str\n",
    "                    query_json[\"dist\"] = dist\n",
    "                    keep_trying = True\n",
    "                    retry_counter = 0\n",
    "                    while keep_trying:\n",
    "                        try:\n",
    "                            resp_json = requests.post(backend_url + \"phrase\", json=query_json, timeout=90).json()\n",
    "                            assert resp_json[\"err_msg\"] == \"No error\", f\"{query_str}\\n\"+ resp_json[\"err_msg\"]\n",
    "                            keep_trying = False\n",
    "                        except Exception as e:\n",
    "                            retry_counter += 1\n",
    "                            if retry_counter > 10:\n",
    "                                raise e\n",
    "                            time.sleep(1)\n",
    "                    del resp_json[\"books\"]\n",
    "                    record.append(resp_json)\n",
    "                    pbar.update()\n",
    "                    time.sleep(0.2)\n",
    "\n",
    "def test_multi_lingual(rounds: int, dist: int, datasets: dict):\n",
    "    total = sum(len(v) for v in datasets.values()) * rounds\n",
    "    with tqdm(total=total, ncols=80, leave=False) as pbar:\n",
    "        for language, dataset in datasets.items():\n",
    "            requests.post(backend_url + \"setcache\", json={\"clear_cache\" : True}).json()\n",
    "            for _ in range(rounds):\n",
    "                for (query_str, _), record in dataset.items():\n",
    "                    query_json = query_template.copy()\n",
    "                    query_json[\"query\"] = query_str\n",
    "                    query_json[\"dist\"] = dist\n",
    "                    query_json[\"languages\"] = [language]\n",
    "                    keep_trying = True\n",
    "                    retry_counter = 0\n",
    "                    while keep_trying:\n",
    "                        try:\n",
    "                            resp_json = requests.post(backend_url + \"phrase\", json=query_json, timeout=90).json()\n",
    "                            assert resp_json[\"err_msg\"] == \"No error\", f\"{query_str}\\n\"+ resp_json[\"err_msg\"]\n",
    "                            keep_trying = False\n",
    "                        except Exception as e:\n",
    "                            retry_counter += 1\n",
    "                            if retry_counter > 10:\n",
    "                                raise e\n",
    "                            time.sleep(1)\n",
    "                    del resp_json[\"books\"]\n",
    "                    record.append(resp_json)\n",
    "                    pbar.update()\n",
    "                    time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_multi_lingual(10, 3, multi_lingual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_english(10, 3, memorable, generic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_sets/test_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump((generic, memorable, multi_lingual), f, protocol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_as_csv(result_dict, fname):\n",
    "    content = \"query_id,query,number of matches,average response time over 10 rounds (s),minimum response time (s),maximum response time,standard deviation of query response time (ms)\\n\"\n",
    "    for i, (k, v) in enumerate(result_dict.items()):\n",
    "        query_times = np.array([j[\"queryTime\"] for j in v], dtype=np.float64)\n",
    "        num_matches = v[0][\"totalNum\"]\n",
    "        content += f\"{i},\\\"{k}\\\",{num_matches},{query_times.mean()},{query_times.min()},{query_times.max()},{query_times.std() * 100}\\n\"\n",
    "    with open(fname, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_as_csv(generic, \"test_sets/generic_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_as_csv(memorable, \"test_sets/memorable_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, d in multi_lingual.items():\n",
    "    write_as_csv({query : v for (query, translation), v in d.items()}, f\"test_sets/{k.lower()}_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
