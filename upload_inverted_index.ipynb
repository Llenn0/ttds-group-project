{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please ignore the syntax warnings as small integers in CPython are singletons\n",
      "Using `is` instead of `=` for comparison in performance-critical code is acceptable\n",
      "Downloading stopwords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\10022\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import traceback\n",
    "import concurrent.futures\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from KeywordSearch import loader, indexing, utils, kwsearch\n",
    "from KeywordSearch.cloud_index import prepare_tokendict_for_upload, upload_firestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import firestore\n",
    "db = firestore.Client(project=\"moonlit-oven-412316\")\n",
    "index_api = db.collection(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_segments = glob(\"*_merged.pkl\", root_dir=loader.index_dir)\n",
    "regex_segment_id = re.compile(\"([0-9]+)_merged.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segment   35/2987:   1%|5                                               | 174499/14930836 [1:28:51<117:13:29, 34.97it/s]"
     ]
    }
   ],
   "source": [
    "finished: dict[int, list] = dict()\n",
    "failed: dict[int, list] = dict()\n",
    "num_segments = len(pickled_segments)\n",
    "with concurrent.futures.ProcessPoolExecutor() as pool:\n",
    "    with tqdm(total=len(loader.all_tokens), desc=\"Initializing\", ncols=120, ascii=True) as pbar:\n",
    "        for counter, fname in enumerate(pickled_segments, 1):\n",
    "            segment_id = int(regex_segment_id.fullmatch(fname).group(1))\n",
    "            offset = 5000 * segment_id\n",
    "            finished[segment_id] = []\n",
    "            failed[segment_id] = []\n",
    "            with open(os.path.join(loader.index_dir, fname), \"rb\") as f:\n",
    "                current_slice = pickle.load(f)\n",
    "            pbar.set_description(f\"Segment {counter:4d}/{num_segments:4d}\")\n",
    "            jobs = {pool.submit(prepare_tokendict_for_upload, token_dict, i) : i \n",
    "                    for i, token_dict in enumerate(current_slice, start=offset)}\n",
    "            for job in concurrent.futures.as_completed(jobs):\n",
    "                token_id = jobs[job]\n",
    "                try:\n",
    "                    for doc in job.result():\n",
    "                        upload_firestore(doc, index_api)\n",
    "                except Exception as e:\n",
    "                    failed[segment_id].append(token_id)\n",
    "                    with open(loader.LOG_PATH, 'a', encoding=\"UTF-8\") as f:\n",
    "                        f.write(f\"Upload failure at {token_id}:\\n{''.join(traceback.format_exception(e))}\\n\")\n",
    "                finished[segment_id].append(token_id)\n",
    "                pbar.update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
