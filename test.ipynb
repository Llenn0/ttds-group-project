{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10022\\Documents\\GitHub\\ttds-group-project\\KeywordSearch\\loader.py:174: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if (complete_counter % 500 is 0):\n",
      "c:\\Users\\10022\\Documents\\GitHub\\ttds-group-project\\KeywordSearch\\loader.py:183: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if (complete_counter % 500 is 0):\n",
      "c:\\Users\\10022\\Documents\\GitHub\\ttds-group-project\\KeywordSearch\\indexing.py:134: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if complete_counter % 5 is 0: gc.collect()\n"
     ]
    }
   ],
   "source": [
    "from KeywordSearch import loader, indexing, utils\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_size = 5000\n",
    "part_size = 5000\n",
    "\n",
    "# for i in range(13):\n",
    "#     gc.collect()\n",
    "#     print(f\"\\nPart {i:02d}\", flush=True)\n",
    "#     indexing.build_full_index(offset=i*part_size, k=part_size, batch_size=segment_size, index_type=\"inverted\", prefix=f\"part{i:02d}\", unsafe_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sizes = utils.measure_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(sizes.items())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing.merge_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140 segments to load\n",
      "Finished merging 1140 segments...\n",
      "Garbage collection done\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "index = loader.load_merged_index(save_merged=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44GB 370MB\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "from pympler.asizeof import asizeof\n",
    "index_size = asizeof(index)\n",
    "mb = 1024 * 1024\n",
    "gb = mb * 1024\n",
    "gb_size = index_size // gb\n",
    "mb_size = round((index_size % gb) / mb)\n",
    "print(f\"{gb_size}GB\" + f\" {mb_size}MB\" if mb_size else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = loader.load_token_vocab(k=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"valid_books.pkl\", \"rb\") as f:\n",
    "    _, _, valid_books = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5696023, 5696023)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tokens), len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def construct_bool_table(index: Iterable[dict]):\n",
    "    table = scipy.sparse.dok_matrix((len(all_tokens), max(valid_books) + 1), dtype=np.bool_)\n",
    "    print(len(index), table.shape)\n",
    "    length = len(all_tokens)\n",
    "    for token_id, token_dict in tqdm(enumerate(index[:length]), total=length):\n",
    "        if token_dict:\n",
    "            table[token_id, tuple(token_dict.keys())] = True\n",
    "    gc.collect()\n",
    "    return table.tocsc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5696023 (5696023, 72533)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc48bab05af4c6f93cf6568a7976fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5696023 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = construct_bool_table(index)\n",
    "scipy.sparse.save_npz(\"lookup_table.npz\", table, compressed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1063"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(table.data.nbytes + table.indptr.nbytes + table.indices.nbytes) // 1024 // 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dok_table: scipy.sparse.dok_matrix = table.todok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "asizeof(dok_table) // 1024 // 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asizeof(table) // 1024 // 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz(\"lookup_table_coo.npz\", table.tocoo(), compressed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 5695000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = list(range(5696023))\n",
    "y = []\n",
    "index_size = len(x)\n",
    "batch_size = 5000\n",
    "batches = index_size // batch_size\n",
    "for i in range(batches):\n",
    "    start = i * batch_size\n",
    "    end = min(start + batch_size, index_size)\n",
    "    y.extend(x[start:end])\n",
    "\n",
    "y.extend(x[end:])\n",
    "x == y, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5696023"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"all_tokens_old.pkl\", 'rb') as f:\n",
    "    _, _, all_tokens_old = pickle.load(f)\n",
    "len(all_tokens_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverted_index_2, indexed_books_2 = indexing.build_full_index(offset=5000, k=5000, batch_size=-1, index_type=\"inverted\", prefix=\"part02\")\n",
    "# import h5py\n",
    "# def save_inv_index_HDF5(filename: str, index: list[dict], **kwargs):\n",
    "#     with h5py.File(filename, 'w') as f:\n",
    "#         for i, entry in enumerate(index):\n",
    "#             group = f.create_group(str(i))\n",
    "#             for book_id, occurrences in entry.items():\n",
    "#                 group.create_dataset(str(book_id), data=occurrences, **kwargs)\n",
    "# save_inv_index_HDF5(\"test.h5\", inverted_index_1, chunks=True, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(\"index/inverted_113.pkl\", \"rb\") as f:\n",
    "    tmp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bow_index, indexed_books = indexing.build_full_index(k=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(\"index.pkl\", \"rb\") as f:\n",
    "    bow_index = pickle.load(f)\n",
    "# dummy_arr = np.array([], dtype=np.uint8)\n",
    "# dummy = (0, dummy_arr, 0, dummy_arr)\n",
    "# bow_index = tuple([dummy] + [pair if isinstance(pair, tuple) else dummy for pair in bow_index[1:]])\n",
    "# with open(\"index.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(bow_index, f)\n",
    "\n",
    "with open(\"valid_books.pkl\", \"rb\") as f:\n",
    "    _, _, valid_books = pickle.load(f)\n",
    "\n",
    "book_list = sorted(valid_books)\n",
    "\n",
    "non_empty_books = []\n",
    "non_empty_index = []\n",
    "for book_id, book_bow in enumerate(bow_index):\n",
    "    if book_bow:\n",
    "        non_empty_books.append(book_id)\n",
    "        non_empty_index.append(book_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import MatrixSimilarity, SparseMatrixSimilarity\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [utils.deltazip([vocab, counts], [vocab_delta, count_delta]) for vocab_delta, vocab, count_delta, counts in non_empty_index]\n",
    "# tfidf_model = TfidfModel(corpus, smartirs='ntc')\n",
    "# with open(\"tfidf_model.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(tfidf_model, f)\n",
    "with open(\"tfidf_model.pkl\", \"rb\") as f:\n",
    "    tfidf_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(all_tokens) + 1\n",
    "def cast2numpy(x):\n",
    "    arr = np.zeros(dim, dtype=np.float16)\n",
    "    for i, score in x:\n",
    "        arr[i] = score\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [tfidf_model[utils.deltazip([vocab, counts], [vocab_delta, count_delta])] for vocab_delta, vocab, count_delta, counts in non_empty_index]\n",
    "# with open(\"tfidf_index.pkl\", \"rb\") as f:\n",
    "#     tmp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model[[(0,1),(3,8)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_delta, vocab, count_delta, counts = non_empty_index[0]\n",
    "cast2numpy(tfidf_model[utils.deltazip([vocab, counts], [vocab_delta, count_delta])]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(all_tokens) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = SparseMatrixSimilarity(tmp, num_best=10, num_features=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tfidf_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tmp, f)\n",
    "with open(\"matrix.pkl\", \"wb\") as f:\n",
    "    pickle.dump(m1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asizeof.asizeof(tmp)/1024/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asizeof.asizeof(m1)/1024/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_delta, vocab, count_delta, counts = non_empty_index[269]\n",
    "next(utils.deltazip([vocab, counts], [vocab_delta, count_delta]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty_index[269]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [1, 3, 1, 9, 3, 5654]\n",
    "vocab_list = sorted(set(tokens))\n",
    "token_arr = np.array(tokens)\n",
    "vocab, vocab_delta = utils.cast2intarr(vocab_list)\n",
    "counts, count_delta = utils.cast2intarr([np.sum(token_arr == token) for token in vocab_list])\n",
    "(vocab_delta, vocab, count_delta, counts, vocab_list, [(vocab == token-vocab_delta) for token in vocab_list])\n",
    "vocab + vocab_delta, counts + count_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_path_template = loader.token_dir + \"PG%d_tokens.txt\"\n",
    "\n",
    "def read_tokens(PG_id: int):\n",
    "    with open(book_path_template %PG_id, encoding=\"UTF-8\", errors=\"ignore\") as f:\n",
    "        return loader.stemmer.stemWords(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "docs = []\n",
    "docs_index = []\n",
    "failed_jobs = 0\n",
    "complete_counter = 0\n",
    "with concurrent.futures.ThreadPoolExecutor() as pool:\n",
    "    jobs = {\n",
    "        pool.submit(\n",
    "            read_tokens, book_id)\n",
    "            : book_id for book_id in indexed_books\n",
    "        }\n",
    "    \n",
    "    for job in concurrent.futures.as_completed(jobs):\n",
    "        book_id = jobs[job]\n",
    "        try:\n",
    "            result = job.result()\n",
    "            docs.append(result)\n",
    "            docs_index.append(book_id)\n",
    "        except Exception as e:\n",
    "            # raise e\n",
    "            failed_jobs.append(book_id)\n",
    "        complete_counter += 1\n",
    "        print(f\"Finished fetching tokens in {complete_counter} books...\", end=\"\\r\")\n",
    "        \n",
    "        print(f\"\\n{len(failed_jobs)}/{len(jobs)} token fetching jobs failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "del docs\n",
    "tfidf = TfidfModel(corpus, smartirs='ntc')\n",
    "tfidf_corpus = [tfidf[doc] for doc in corpus]\n",
    "del corpus, tfidf\n",
    "index = MatrixSimilarity(tfidf_corpus, num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"all_tokens.pkl\", 'rb') as f:\n",
    "#     k, offset, _all_tokens = pickle.load(f)\n",
    "# with open(\"all_tokens.pkl\", \"wb\") as f:\n",
    "#     pickle.dump((k, offset, tuple(_all_tokens)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import asizeof\n",
    "asizeof.asizeof(bow_index), asizeof.asizeof(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "415963552/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "415971552/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mem_size_lst = asizeof.asizeof(all_tokens)\n",
    "mem_size_tup = asizeof.asizeof(tuple(all_tokens))\n",
    "mem_size_lst/1024/1024, mem_size_tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "import numpy as np\n",
    "log2(len(all_tokens))\n",
    "x = np.array([1,2,3])\n",
    "x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader.build_full_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"index.pkl\", \"rb\") as f:\n",
    "    index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import asizeof\n",
    "mem_size = asizeof.asizeof(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_size/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0\n",
    "print(len(index))\n",
    "for i, token in enumerate(index):\n",
    "    for arr in token.values():\n",
    "        size += asizeof.asizeof(arr)\n",
    "    print(i, end='\\r')\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size/1024/1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
