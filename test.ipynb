{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KeywordSearch import loader, indexing, utils\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader.process_first_k_books(k=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = loader.load_token_vocab(k=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building index for 408 books...\r"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "_ = indexing.build_full_index(offset=30000, k=5000, batch_size=1000, index_type=\"inverted\", prefix=\"part07\", unsafe_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building index for 5000 books...\n",
      "0/5000 failures while building index\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "_ = indexing.build_full_index(offset=35000, k=5000, batch_size=1000, index_type=\"inverted\", prefix=\"part08\", unsafe_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "_ = indexing.build_full_index(offset=40000, k=5000, batch_size=1000, index_type=\"inverted\", prefix=\"part09\", unsafe_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building index for 5000 books...\n",
      "0/5000 failures while building index\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "_ = indexing.build_full_index(offset=15000, k=5000, batch_size=1000, index_type=\"inverted\", prefix=\"part04\", unsafe_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building index for 5000 books...\n",
      "0/5000 failures while building index\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "_ = indexing.build_full_index(offset=20000, k=5000, batch_size=1000, index_type=\"inverted\", prefix=\"part05\", unsafe_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverted_index_2, indexed_books_2 = indexing.build_full_index(offset=5000, k=5000, batch_size=-1, index_type=\"inverted\", prefix=\"part02\")\n",
    "import h5py\n",
    "def save_inv_index_HDF5(filename: str, index: list[dict], **kwargs):\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        for i, entry in enumerate(index):\n",
    "            group = f.create_group(str(i))\n",
    "            for book_id, occurrences in entry.items():\n",
    "                group.create_dataset(str(book_id), data=occurrences, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_inv_index_HDF5(\"test.h5\", inverted_index_1, chunks=True, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(\"index/inverted_113.pkl\", \"rb\") as f:\n",
    "    tmp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bow_index, indexed_books = indexing.build_full_index(k=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(\"index.pkl\", \"rb\") as f:\n",
    "    bow_index = pickle.load(f)\n",
    "# dummy_arr = np.array([], dtype=np.uint8)\n",
    "# dummy = (0, dummy_arr, 0, dummy_arr)\n",
    "# bow_index = tuple([dummy] + [pair if isinstance(pair, tuple) else dummy for pair in bow_index[1:]])\n",
    "# with open(\"index.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(bow_index, f)\n",
    "\n",
    "with open(\"valid_books.pkl\", \"rb\") as f:\n",
    "    _, _, valid_books = pickle.load(f)\n",
    "\n",
    "book_list = sorted(valid_books)\n",
    "\n",
    "non_empty_books = []\n",
    "non_empty_index = []\n",
    "for book_id, book_bow in enumerate(bow_index):\n",
    "    if book_bow:\n",
    "        non_empty_books.append(book_id)\n",
    "        non_empty_index.append(book_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import MatrixSimilarity, SparseMatrixSimilarity\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [utils.deltazip([vocab, counts], [vocab_delta, count_delta]) for vocab_delta, vocab, count_delta, counts in non_empty_index]\n",
    "# tfidf_model = TfidfModel(corpus, smartirs='ntc')\n",
    "# with open(\"tfidf_model.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(tfidf_model, f)\n",
    "with open(\"tfidf_model.pkl\", \"rb\") as f:\n",
    "    tfidf_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(all_tokens) + 1\n",
    "def cast2numpy(x):\n",
    "    arr = np.zeros(dim, dtype=np.float16)\n",
    "    for i, score in x:\n",
    "        arr[i] = score\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [tfidf_model[utils.deltazip([vocab, counts], [vocab_delta, count_delta])] for vocab_delta, vocab, count_delta, counts in non_empty_index]\n",
    "# with open(\"tfidf_index.pkl\", \"rb\") as f:\n",
    "#     tmp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model[[(0,1),(3,8)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_delta, vocab, count_delta, counts = non_empty_index[0]\n",
    "cast2numpy(tfidf_model[utils.deltazip([vocab, counts], [vocab_delta, count_delta])]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(all_tokens) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = SparseMatrixSimilarity(tmp, num_best=10, num_features=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tfidf_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tmp, f)\n",
    "with open(\"matrix.pkl\", \"wb\") as f:\n",
    "    pickle.dump(m1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asizeof.asizeof(tmp)/1024/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asizeof.asizeof(m1)/1024/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_delta, vocab, count_delta, counts = non_empty_index[269]\n",
    "next(utils.deltazip([vocab, counts], [vocab_delta, count_delta]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty_index[269]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [1, 3, 1, 9, 3, 5654]\n",
    "vocab_list = sorted(set(tokens))\n",
    "token_arr = np.array(tokens)\n",
    "vocab, vocab_delta = utils.cast2intarr(vocab_list)\n",
    "counts, count_delta = utils.cast2intarr([np.sum(token_arr == token) for token in vocab_list])\n",
    "(vocab_delta, vocab, count_delta, counts, vocab_list, [(vocab == token-vocab_delta) for token in vocab_list])\n",
    "vocab + vocab_delta, counts + count_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_path_template = loader.token_dir + \"PG%d_tokens.txt\"\n",
    "\n",
    "def read_tokens(PG_id: int):\n",
    "    with open(book_path_template %PG_id, encoding=\"UTF-8\", errors=\"ignore\") as f:\n",
    "        return loader.stemmer.stemWords(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "docs = []\n",
    "docs_index = []\n",
    "failed_jobs = 0\n",
    "complete_counter = 0\n",
    "with concurrent.futures.ThreadPoolExecutor() as pool:\n",
    "    jobs = {\n",
    "        pool.submit(\n",
    "            read_tokens, book_id)\n",
    "            : book_id for book_id in indexed_books\n",
    "        }\n",
    "    \n",
    "    for job in concurrent.futures.as_completed(jobs):\n",
    "        book_id = jobs[job]\n",
    "        try:\n",
    "            result = job.result()\n",
    "            docs.append(result)\n",
    "            docs_index.append(book_id)\n",
    "        except Exception as e:\n",
    "            # raise e\n",
    "            failed_jobs.append(book_id)\n",
    "        complete_counter += 1\n",
    "        print(f\"Finished fetching tokens in {complete_counter} books...\", end=\"\\r\")\n",
    "        \n",
    "        print(f\"\\n{len(failed_jobs)}/{len(jobs)} token fetching jobs failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "del docs\n",
    "tfidf = TfidfModel(corpus, smartirs='ntc')\n",
    "tfidf_corpus = [tfidf[doc] for doc in corpus]\n",
    "del corpus, tfidf\n",
    "index = MatrixSimilarity(tfidf_corpus, num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"all_tokens.pkl\", 'rb') as f:\n",
    "#     k, offset, _all_tokens = pickle.load(f)\n",
    "# with open(\"all_tokens.pkl\", \"wb\") as f:\n",
    "#     pickle.dump((k, offset, tuple(_all_tokens)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import asizeof\n",
    "asizeof.asizeof(bow_index), asizeof.asizeof(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "415963552/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "415971552/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mem_size_lst = asizeof.asizeof(all_tokens)\n",
    "mem_size_tup = asizeof.asizeof(tuple(all_tokens))\n",
    "mem_size_lst/1024/1024, mem_size_tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "import numpy as np\n",
    "log2(len(all_tokens))\n",
    "x = np.array([1,2,3])\n",
    "x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader.build_full_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"index.pkl\", \"rb\") as f:\n",
    "    index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import asizeof\n",
    "mem_size = asizeof.asizeof(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_size/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0\n",
    "print(len(index))\n",
    "for i, token in enumerate(index):\n",
    "    for arr in token.values():\n",
    "        size += asizeof.asizeof(arr)\n",
    "    print(i, end='\\r')\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size/1024/1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
