{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10022\\Documents\\GitHub\\ttds-group-project\\KeywordSearch\\loader.py:200: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if (complete_counter % 100 is 0):\n",
      "c:\\Users\\10022\\Documents\\GitHub\\ttds-group-project\\KeywordSearch\\loader.py:209: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if (complete_counter % 100 is 0):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please ignore the syntax warnings as small integers in CPython are singletons\n",
      "Using `is` instead of `=` for comparison in performance-critical code is acceptable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10022\\Documents\\GitHub\\ttds-group-project\\KeywordSearch\\indexing.py:134: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if complete_counter % 5 is 0: gc.collect()\n"
     ]
    }
   ],
   "source": [
    "from KeywordSearch import loader, indexing, utils, kwsearch\n",
    "import gc\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fine AND ', 'okay AND (good OR NOT great)', ' AND', ' happy']\n",
      "['okay AND ', 'good OR NOT great', None, '']\n"
     ]
    }
   ],
   "source": [
    "and_set = indexing.stemmer.stemWords([\"fine\", \"okay\", \"happy\"])\n",
    "good = indexing.stemmer.stemWord(\"good\")\n",
    "not_great = indexing.stemmer.stemWord(\"great\")\n",
    "query = \"fine AND (okay AND (good OR NOT great)) AND happy\"\n",
    "result, _ = kwsearch.bool_search(query, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05207a57f543469abb8dc54e251d190f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating 32 results:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_result = len(result)\n",
    "for book_id in tqdm(result, total=num_result, desc=f\"Validating {num_result} results\"):\n",
    "    with open(loader.token_dir + f\"PG{book_id}_tokens.txt\", 'r', encoding=\"UTF-8\", errors=\"ignore\") as f:\n",
    "        tokens = set(indexing.stemmer.stemWords(f.read().splitlines()))\n",
    "    assert all((term in tokens) for term in and_set) and ((good in tokens) or (not_great not in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 0.0063772 s\n",
      "File: c:\\Users\\10022\\Documents\\GitHub\\ttds-group-project\\KeywordSearch\\kwsearch.py\n",
      "Function: bool_search at line 49\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    49                                           def bool_search(query: str, debug: bool=False) -> set:\n",
      "    50         2          8.0      4.0      0.0      if debug:\n",
      "    51                                                   print(regex_bracket.split(query))\n",
      "    52         2         81.0     40.5      0.1      tokens = (bool_search_atomic(token, debug) for token in regex_bracket.split(query) if token)\n",
      "    53         2         10.0      5.0      0.0      is_not = is_and = is_or = False\n",
      "    54         2          4.0      2.0      0.0      not_first = False\n",
      "    55         2      21981.0  10990.5     34.5      valid, (is_not, is_and, is_or) = next(tokens)\n",
      "    56         6      36527.0   6087.8     57.3      for token_eval, (is_not_, is_and_, is_or_) in tokens:\n",
      "    57         4         31.0      7.8      0.0          is_not |= is_not_; is_and |= is_and_; is_or |= is_or_\n",
      "    58         4         63.0     15.8      0.1          if isinstance(token_eval, list):\n",
      "    59         1          1.0      1.0      0.0              continue\n",
      "    60                                                   # print(f\"Parse: {len(token_eval)} {'OR' if is_or else ''} {'AND' if is_and else ''} {'NOT' if is_not else ''}\")\n",
      "    61         3          7.0      2.3      0.0          if is_or:\n",
      "    62                                                       if is_not:\n",
      "    63                                                           if not_first:\n",
      "    64                                                               valid = token_eval | (all_elems_set - valid) \n",
      "    65                                                           else:\n",
      "    66                                                               valid |= (all_elems_set - token_eval)\n",
      "    67                                                           is_not = False\n",
      "    68                                                       else:\n",
      "    69                                                           valid |= token_eval\n",
      "    70                                                       is_or = is_not = False\n",
      "    71         3          8.0      2.7      0.0          elif is_and:\n",
      "    72         3          7.0      2.3      0.0              if is_not:\n",
      "    73         1          2.0      2.0      0.0                  if not_first:\n",
      "    74                                                               valid = token_eval - valid\n",
      "    75                                                           else:\n",
      "    76         1        882.0    882.0      1.4                      valid -= token_eval\n",
      "    77         1          9.0      9.0      0.0                  is_not = False\n",
      "    78                                                       else:\n",
      "    79         2       4127.0   2063.5      6.5                  valid &= token_eval\n",
      "    80         3         16.0      5.3      0.0              is_and = is_not = False\n",
      "    81                                                   elif is_not:\n",
      "    82                                                       valid -= token_eval\n",
      "    83                                                   else:\n",
      "    84                                                       print(\"Grammar error?\")\n",
      "    85         2          8.0      4.0      0.0      return valid, (is_not, is_and, is_or)"
     ]
    }
   ],
   "source": [
    "%lprun -f kwsearch.bool_search kwsearch.bool_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_size = 5000\n",
    "part_size = 5000\n",
    "\n",
    "# for i in range(13):\n",
    "#     gc.collect()\n",
    "#     print(f\"\\nPart {i:02d}\", flush=True)\n",
    "#     indexing.build_full_index(offset=i*part_size, k=part_size, batch_size=segment_size, index_type=\"inverted\", prefix=f\"part{i:02d}\", unsafe_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140 segments to load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1140/1140 [05:25<00:00,  3.51it/s]\n",
      "100%|██████████| 1140/1140 [00:05<00:00, 227.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Garbage collection done\n",
      "The index took 5 minutes  32 seconds to load\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "index = loader.load_merged_index(save_merged=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10007, 37174}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_query = \"Upon a paper attached to the Narrative\".casefold().split(' ')\n",
    "kwsearch.phrase_search(phrase_query, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 0.325882 s\n",
      "File: c:\\Users\\10022\\Documents\\GitHub\\ttds-group-project\\KeywordSearch\\kwsearch.py\n",
      "Function: phrase_search at line 144\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   144                                           def phrase_search(words: list[str], index: list[dict] | tuple[dict], debug: bool=False):\n",
      "   145         1          3.0      3.0      0.0      search_result = []\n",
      "   146         1          4.0      4.0      0.0      if debug:\n",
      "   147                                                   print([stemmer.stemWord(word) for word in words if word not in indexing.stopwords_set])\n",
      "   148         1        183.0    183.0      0.0      word_ids = [token_index_dict[stemmer.stemWord(word)] for word in words if word not in indexing.stopwords_set]\n",
      "   149         1         41.0     41.0      0.0      index_entries = [index[i] for i in word_ids]\n",
      "   150         1         15.0     15.0      0.0      first = list(set(word_ids))\n",
      "   151         1     112274.0 112274.0      3.4      intersection = lookup_table[first.pop(), :].indices\n",
      "   152         4         38.0      9.5      0.0      for token_id in first:\n",
      "   153         3     279786.0  93262.0      8.6          intersection = np.intersect1d(intersection, lookup_table[token_id, :].indices, assume_unique=True)\n",
      "   154         1         11.0     11.0      0.0      del first, word_ids\n",
      "   155                                           \n",
      "   156     23579      58195.0      2.5      1.8      for docID in intersection:\n",
      "   157     23578     231623.0      9.8      7.1          occurs = (entry[docID] for entry in index_entries) # use generator to avoid wasting time on non-matches\n",
      "   158     23578     204559.0      8.7      6.3          first = next(occurs)\n",
      "   159     23578     175554.0      7.4      5.4          second = next(occurs)\n",
      "   160     23578    1803023.0     76.5     55.3          matches: np.ndarray = second[first[np.searchsorted(first, second, side=\"right\")-1] == second-1]\n",
      "   161     23578      42215.0      1.8      1.3          del first, second\n",
      "   162                                           \n",
      "   163     23578      71023.0      3.0      2.2          if matches.shape[0]:\n",
      "   164      2218      22203.0     10.0      0.7              for entry in occurs:\n",
      "   165      2216      95770.0     43.2      2.9                  i = np.searchsorted(matches, entry, side=\"right\")\n",
      "   166      2216      39345.0     17.8      1.2                  i[i==0] = 1\n",
      "   167      2216      69912.0     31.5      2.1                  matches = entry[matches[i-1] == entry-1]\n",
      "   168      2216      48956.0     22.1      1.5                  if not matches.any():\n",
      "   169      2211       4045.0      1.8      0.1                      break\n",
      "   170                                                       else:\n",
      "   171         2         25.0     12.5      0.0                  search_result.append(docID)\n",
      "   172         1         21.0     21.0      0.0      return set(search_result)"
     ]
    }
   ],
   "source": [
    "%lprun -f kwsearch.phrase_search kwsearch.phrase_search(phrase_query, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KeywordSearch.kwsearch import stemmer, token_index_dict, lookup_table, book_index\n",
    "from KeywordSearch.indexing import stopwords_set\n",
    "import numpy as np\n",
    "\n",
    "def phrase_search(words: list[str], debug: bool=False):\n",
    "    search_result = []\n",
    "    if debug:\n",
    "        print([stemmer.stemWord(word) for word in words if word not in stopwords_set])\n",
    "    word_ids = [token_index_dict[stemmer.stemWord(word)] for word in words if word not in stopwords_set]\n",
    "    index_entries = [index[i] for i in word_ids]\n",
    "    first = list(set(word_ids))\n",
    "    intersection = lookup_table[first.pop(), :].indices\n",
    "    for token_id in first:\n",
    "        intersection = np.intersect1d(intersection, lookup_table[token_id, :].indices, assume_unique=True)\n",
    "    del first, word_ids\n",
    "\n",
    "    for docID in intersection:\n",
    "        occurs = (entry[docID] for entry in index_entries) # use generator to avoid wasting time on non-matches\n",
    "        first = next(occurs)\n",
    "        second = next(occurs)\n",
    "        matches: np.ndarray = second[first[np.searchsorted(first, second, side=\"right\")-1] == second-1]\n",
    "        del first, second\n",
    "\n",
    "        if matches.shape[0]:\n",
    "            for entry in occurs:\n",
    "                i = np.searchsorted(matches, entry, side=\"right\")\n",
    "                i[i==0] = 1\n",
    "                matches = entry[matches[i-1] == entry-1]\n",
    "                if not matches.any():\n",
    "                    break\n",
    "            else:\n",
    "                search_result.append(docID)\n",
    "    return set(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10007, 37174}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_search(\"Upon a paper attached to the Narrative\".casefold().split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44GB 370MB\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "from pympler.asizeof import asizeof\n",
    "index_size = asizeof(index)\n",
    "mb = 1024 * 1024\n",
    "gb = mb * 1024\n",
    "gb_size = index_size // gb\n",
    "mb_size = round((index_size % gb) / mb)\n",
    "print(f\"{gb_size}GB\" + f\" {mb_size}MB\" if mb_size else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = loader.load_token_vocab(k=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"valid_books.pkl\", \"rb\") as f:\n",
    "    _, _, valid_books = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5696023, 5696023)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tokens), len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    USE_TQDM = True\n",
    "except:\n",
    "    USE_TQDM = False\n",
    "\n",
    "def construct_bool_table(index: Iterable[dict], save_path: str|None=None):\n",
    "    table = scipy.sparse.dok_matrix((len(all_tokens), max(valid_books) + 1), dtype=np.bool_)\n",
    "    print(len(index), table.shape)\n",
    "    length = len(all_tokens)\n",
    "    tqdm_iter = enumerate(index[:length])\n",
    "    if USE_TQDM:\n",
    "        tqdm_iter = tqdm(tqdm_iter, total=length)\n",
    "    for token_id, token_dict in tqdm_iter:\n",
    "        if token_dict:\n",
    "            table[token_id, tuple(token_dict.keys())] = True\n",
    "    gc.collect()\n",
    "    table = table.tocsr()\n",
    "    if save_path is None:\n",
    "        return table\n",
    "    else:\n",
    "        gc.collect()\n",
    "        scipy.sparse.save_npz(\"lookup_table.npz\", table, compressed=True)\n",
    "        return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5696023 (5696023, 72533)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc48bab05af4c6f93cf6568a7976fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5696023 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = construct_bool_table(index)\n",
    "scipy.sparse.save_npz(\"lookup_table.npz\", table, compressed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1063"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(table.data.nbytes + table.indptr.nbytes + table.indices.nbytes) // 1024 // 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dok_table: scipy.sparse.dok_matrix = table.todok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asizeof(dok_table) // 1024 // 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asizeof(table) // 1024 // 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz(\"lookup_table_coo.npz\", table.tocoo(), compressed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 5695000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = list(range(5696023))\n",
    "y = []\n",
    "index_size = len(x)\n",
    "batch_size = 5000\n",
    "batches = index_size // batch_size\n",
    "for i in range(batches):\n",
    "    start = i * batch_size\n",
    "    end = min(start + batch_size, index_size)\n",
    "    y.extend(x[start:end])\n",
    "\n",
    "y.extend(x[end:])\n",
    "x == y, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5696023"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"all_tokens_old.pkl\", 'rb') as f:\n",
    "    _, _, all_tokens_old = pickle.load(f)\n",
    "len(all_tokens_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverted_index_2, indexed_books_2 = indexing.build_full_index(offset=5000, k=5000, batch_size=-1, index_type=\"inverted\", prefix=\"part02\")\n",
    "# import h5py\n",
    "# def save_inv_index_HDF5(filename: str, index: list[dict], **kwargs):\n",
    "#     with h5py.File(filename, 'w') as f:\n",
    "#         for i, entry in enumerate(index):\n",
    "#             group = f.create_group(str(i))\n",
    "#             for book_id, occurrences in entry.items():\n",
    "#                 group.create_dataset(str(book_id), data=occurrences, **kwargs)\n",
    "# save_inv_index_HDF5(\"test.h5\", inverted_index_1, chunks=True, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(\"index/inverted_113.pkl\", \"rb\") as f:\n",
    "    first = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bow_index, indexed_books = indexing.build_full_index(k=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(\"index.pkl\", \"rb\") as f:\n",
    "    bow_index = pickle.load(f)\n",
    "# dummy_arr = np.array([], dtype=np.uint8)\n",
    "# dummy = (0, dummy_arr, 0, dummy_arr)\n",
    "# bow_index = tuple([dummy] + [pair if isinstance(pair, tuple) else dummy for pair in bow_index[1:]])\n",
    "# with open(\"index.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(bow_index, f)\n",
    "\n",
    "with open(\"valid_books.pkl\", \"rb\") as f:\n",
    "    _, _, valid_books = pickle.load(f)\n",
    "\n",
    "book_list = sorted(valid_books)\n",
    "\n",
    "non_empty_books = []\n",
    "non_empty_index = []\n",
    "for book_id, book_bow in enumerate(bow_index):\n",
    "    if book_bow:\n",
    "        non_empty_books.append(book_id)\n",
    "        non_empty_index.append(book_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import MatrixSimilarity, SparseMatrixSimilarity\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [utils.deltazip([vocab, counts], [vocab_delta, count_delta]) for vocab_delta, vocab, count_delta, counts in non_empty_index]\n",
    "# tfidf_model = TfidfModel(corpus, smartirs='ntc')\n",
    "# with open(\"tfidf_model.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(tfidf_model, f)\n",
    "with open(\"tfidf_model.pkl\", \"rb\") as f:\n",
    "    tfidf_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(all_tokens) + 1\n",
    "def cast2numpy(x):\n",
    "    arr = np.zeros(dim, dtype=np.float16)\n",
    "    for i, score in x:\n",
    "        arr[i] = score\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = [tfidf_model[utils.deltazip([vocab, counts], [vocab_delta, count_delta])] for vocab_delta, vocab, count_delta, counts in non_empty_index]\n",
    "# with open(\"tfidf_index.pkl\", \"rb\") as f:\n",
    "#     tmp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model[[(0,1),(3,8)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_delta, vocab, count_delta, counts = non_empty_index[0]\n",
    "cast2numpy(tfidf_model[utils.deltazip([vocab, counts], [vocab_delta, count_delta])]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(all_tokens) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = SparseMatrixSimilarity(first, num_best=10, num_features=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tfidf_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(first, f)\n",
    "with open(\"matrix.pkl\", \"wb\") as f:\n",
    "    pickle.dump(m1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asizeof.asizeof(first)/1024/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asizeof.asizeof(m1)/1024/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_delta, vocab, count_delta, counts = non_empty_index[269]\n",
    "next(utils.deltazip([vocab, counts], [vocab_delta, count_delta]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty_index[269]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [1, 3, 1, 9, 3, 5654]\n",
    "vocab_list = sorted(set(tokens))\n",
    "token_arr = np.array(tokens)\n",
    "vocab, vocab_delta = utils.cast2intarr(vocab_list)\n",
    "counts, count_delta = utils.cast2intarr([np.sum(token_arr == token) for token in vocab_list])\n",
    "(vocab_delta, vocab, count_delta, counts, vocab_list, [(vocab == token-vocab_delta) for token in vocab_list])\n",
    "vocab + vocab_delta, counts + count_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_path_template = loader.token_dir + \"PG%d_tokens.txt\"\n",
    "\n",
    "def read_tokens(PG_id: int):\n",
    "    with open(book_path_template %PG_id, encoding=\"UTF-8\", errors=\"ignore\") as f:\n",
    "        return loader.stemmer.stemWords(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "docs = []\n",
    "docs_index = []\n",
    "failed_jobs = 0\n",
    "complete_counter = 0\n",
    "with concurrent.futures.ThreadPoolExecutor() as pool:\n",
    "    jobs = {\n",
    "        pool.submit(\n",
    "            read_tokens, book_id)\n",
    "            : book_id for book_id in indexed_books\n",
    "        }\n",
    "    \n",
    "    for job in concurrent.futures.as_completed(jobs):\n",
    "        book_id = jobs[job]\n",
    "        try:\n",
    "            result = job.result()\n",
    "            docs.append(result)\n",
    "            docs_index.append(book_id)\n",
    "        except Exception as e:\n",
    "            # raise e\n",
    "            failed_jobs.append(book_id)\n",
    "        complete_counter += 1\n",
    "        print(f\"Finished fetching tokens in {complete_counter} books...\", end=\"\\r\")\n",
    "        \n",
    "        print(f\"\\n{len(failed_jobs)}/{len(jobs)} token fetching jobs failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "del docs\n",
    "tfidf = TfidfModel(corpus, smartirs='ntc')\n",
    "tfidf_corpus = [tfidf[doc] for doc in corpus]\n",
    "del corpus, tfidf\n",
    "index = MatrixSimilarity(tfidf_corpus, num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"all_tokens.pkl\", 'rb') as f:\n",
    "#     k, offset, _all_tokens = pickle.load(f)\n",
    "# with open(\"all_tokens.pkl\", \"wb\") as f:\n",
    "#     pickle.dump((k, offset, tuple(_all_tokens)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import asizeof\n",
    "asizeof.asizeof(bow_index), asizeof.asizeof(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "415963552/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "415971552/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mem_size_lst = asizeof.asizeof(all_tokens)\n",
    "mem_size_tup = asizeof.asizeof(tuple(all_tokens))\n",
    "mem_size_lst/1024/1024, mem_size_tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "import numpy as np\n",
    "log2(len(all_tokens))\n",
    "x = np.array([1,2,3])\n",
    "x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader.build_full_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"index.pkl\", \"rb\") as f:\n",
    "    index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import asizeof\n",
    "mem_size = asizeof.asizeof(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_size/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0\n",
    "print(len(index))\n",
    "for i, token_eval in enumerate(index):\n",
    "    for arr in token_eval.values():\n",
    "        size += asizeof.asizeof(arr)\n",
    "    print(i, end='\\r')\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size/1024/1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
