{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KeywordSearch import loader, indexing, utils, kwsearch\n",
    "import gc\n",
    "import scipy.sparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\n"
     ]
    }
   ],
   "source": [
    "print(len(kwsearch.bool_search(\"fine AND (okay AND good) AND happy AND fun\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 0.004923 s\n",
      "File: c:\\Users\\10022\\Documents\\GitHub\\ttds-group-project\\KeywordSearch\\kwsearch.py\n",
      "Function: bool_search at line 33\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    33                                           def bool_search(query: str) -> set:\n",
      "    34                                               # print(regex_bracket.split(query))\n",
      "    35         1         41.0     41.0      0.1      tokens = (bool_search_atomic(token) for token in regex_bracket.split(query) if token)\n",
      "    36         1          2.0      2.0      0.0      is_not = is_and = is_or = False\n",
      "    37         1          2.0      2.0      0.0      not_first = False\n",
      "    38         1      17233.0  17233.0     35.0      valid, (is_not, is_and, is_or) = next(tokens)\n",
      "    39         4      27058.0   6764.5     55.0      for token_eval, (is_not_, is_and_, is_or_) in tokens:\n",
      "    40         3         15.0      5.0      0.0          is_not |= is_not_; is_and |= is_and_; is_or |= is_or_\n",
      "    41         3         24.0      8.0      0.0          if isinstance(token_eval, list):\n",
      "    42         1          2.0      2.0      0.0              continue\n",
      "    43                                                   # print(f\"Parse: {len(token_eval)} {'OR' if is_or else ''} {'AND' if is_and else ''} {'NOT' if is_not else ''}\")\n",
      "    44         2          4.0      2.0      0.0          if is_or:\n",
      "    45                                                       if is_not:\n",
      "    46                                                           if not_first:\n",
      "    47                                                               valid = token_eval | (all_elems_set - valid) \n",
      "    48                                                           else:\n",
      "    49                                                               valid |= (all_elems_set - token_eval)\n",
      "    50                                                           is_not = False\n",
      "    51                                                       else:\n",
      "    52                                                           valid |= token_eval\n",
      "    53                                                       is_or = is_not = False\n",
      "    54         2          5.0      2.5      0.0          elif is_and:\n",
      "    55         2          4.0      2.0      0.0              if is_not:\n",
      "    56                                                           if not_first:\n",
      "    57                                                               valid = token_eval - valid\n",
      "    58                                                           else:\n",
      "    59                                                               valid -= token_eval\n",
      "    60                                                           is_not = False\n",
      "    61                                                       else:\n",
      "    62         2       4815.0   2407.5      9.8                  valid &= token_eval\n",
      "    63         2         23.0     11.5      0.0              is_and = is_not = False\n",
      "    64                                                   elif is_not:\n",
      "    65                                                       valid -= token_eval\n",
      "    66                                                   else:\n",
      "    67                                                       print(\"Grammar error?\")\n",
      "    68         1          2.0      2.0      0.0      return valid"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "\n",
    "%lprun -f kwsearch.bool_search kwsearch.bool_search(\"fine AND (okay AND good) AND happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=uint32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwsearch.book_index[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {kwsearch.all_tokens[i] for i, token in enumerate(np.ravel(kwsearch.lookup_table[:, 208].toarray())) if i and token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KeywordSearch.kwsearch import stemmer, token_index_dict, lookup_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  648,   662,   669,   673,  1838,  3141,  4854,  4860,  4885,\n",
       "        4900,  5242,  5245,  5678,  5680,  5772,  6344,  7488,  9823,\n",
       "       10357, 10657, 10660, 10806, 11397, 12078, 12770, 14019, 14587,\n",
       "       14672, 14754, 16464, 16479, 20096, 22693, 25761, 27348, 27509,\n",
       "       27558, 27559, 27560, 27638, 28189, 28413, 29077, 29233, 29386,\n",
       "       29765, 32030, 32767, 33239, 33439, 33504, 34081, 34122, 34853,\n",
       "       35829, 35830, 36595, 37665, 38041, 38877, 39500, 39775, 40860,\n",
       "       41073, 41409, 42329, 42715, 43060, 43416, 43626, 43705, 45781,\n",
       "       46354, 46565, 46917, 46937, 47787, 48952, 49608, 50292, 51134,\n",
       "       52873, 53793, 53935, 54616, 54966, 55025, 55819, 55989, 56631,\n",
       "       58602, 61956, 62119, 64866, 69018, 70419, 71258, 71431])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_table[1, slice(None)].indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.6 µs ± 1.83 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _, valid = lookup_table[1, :].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.7 µs ± 832 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _, valid = np.nonzero(lookup_table[1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.3 µs ± 711 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit lookup_table[1, :].indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 µs ± 2.09 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit set(lookup_table[1, :].indices) | set(lookup_table[2, :].indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.1 µs ± 1.33 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit set(lookup_table[1, :].indices.tolist() + lookup_table[2, :].indices.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365 µs ± 7.63 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit lookup_table[1, list(set(lookup_table[1, :].indices.tolist() + lookup_table[2, :].indices.tolist()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366 µs ± 7.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit lookup_table[1, tuple(set(lookup_table[1, :].indices.tolist() + lookup_table[2, :].indices.tolist()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m lookup_table[\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mindices\n\u001b[1;32m----> 2\u001b[0m x[x\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m|\u001b[39m x\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "x = lookup_table[1, :].indices\n",
    "x[x%2==1 | x%2==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x98 sparse matrix of type '<class 'numpy.bool_'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_table[2, np.logical_or(lookup_table[1, :].indices, lookup_table[1, :].indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   hello '"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stemWord(\"   hello \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "regex_bracket = re.compile(r\"\\((.+)\\)( +(?:NOT|AND|OR))?\")\n",
    "regex_bool_op = re.compile(r\"(NOT|AND|OR)\")\n",
    "\n",
    "operators = frozenset((\"NOT\", \"AND\", \"OR\"))\n",
    "np.ndarray([]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KeywordSearch.kwsearch import stemmer, token_index_dict, lookup_table\n",
    "from KeywordSearch.indexing import stopwords_set\n",
    "\n",
    "all_elems_list = list(range(lookup_table.shape[1]))\n",
    "all_elems_set = frozenset(all_elems_list)\n",
    "all_elems_arr = np.array(all_elems_list, dtype=np.int32)\n",
    "del all_elems_list\n",
    "\n",
    "def bool_search(query: str) -> set:\n",
    "    # print(regex_bracket.split(query))\n",
    "    tokens = (bool_search_atomic(token) for token in regex_bracket.split(query) if token)\n",
    "    is_not = is_and = is_or = False\n",
    "    not_first = False\n",
    "    valid, (is_not, is_and, is_or) = next(tokens)\n",
    "    for token_eval, (is_not_, is_and_, is_or_) in tokens:\n",
    "        is_not |= is_not_; is_and |= is_and_; is_or |= is_or_\n",
    "        if isinstance(token_eval, list):\n",
    "            continue\n",
    "        # print(f\"Parse: {len(token_eval)} {'OR' if is_or else ''} {'AND' if is_and else ''} {'NOT' if is_not else ''}\")\n",
    "        if is_or:\n",
    "            if is_not:\n",
    "                if not_first:\n",
    "                    valid = token_eval | (all_elems_set - valid) \n",
    "                else:\n",
    "                    valid |= (all_elems_set - token_eval)\n",
    "                is_not = False\n",
    "            else:\n",
    "                valid |= token_eval\n",
    "            is_or = is_not = False\n",
    "        elif is_and:\n",
    "            if is_not:\n",
    "                if not_first:\n",
    "                    valid = token_eval - valid\n",
    "                else:\n",
    "                    valid -= token_eval\n",
    "                is_not = False\n",
    "            else:\n",
    "                valid &= token_eval\n",
    "            is_and = is_not = False\n",
    "        elif is_not:\n",
    "            valid -= token_eval\n",
    "        else:\n",
    "            print(\"Grammar error?\")\n",
    "    return valid\n",
    "\n",
    "def bool_search_atomic(query: str) -> set:\n",
    "    query = query.strip()\n",
    "    # print(query)\n",
    "    if not query:\n",
    "        return set(), (False, False, False)\n",
    "    tokens = [token for token in regex_bool_op.split(query) if token]\n",
    "    is_not = is_and = is_or = False\n",
    "    not_first = False\n",
    "    valid = []\n",
    "    for token in tokens:\n",
    "        if token in operators:\n",
    "            if token == \"OR\":\n",
    "                is_or = True\n",
    "                if is_not: not_first = True\n",
    "            elif token == \"NOT\":\n",
    "                is_not = True\n",
    "                not_first = False\n",
    "            elif token == \"AND\":\n",
    "                is_and = True\n",
    "                if is_not: not_first = True\n",
    "        else:\n",
    "            token_id = token_index_dict[stemmer.stemWord(token.strip().casefold())]\n",
    "            if token_id:\n",
    "                token_eval = lookup_table[token_id, :].indices\n",
    "                if is_or:\n",
    "                    if is_not:\n",
    "                        if not_first:\n",
    "                            valid = np.setdiff1d(all_elems_arr, valid, assume_unique=True)\n",
    "                        else:\n",
    "                            token_eval = np.setdiff1d(all_elems_arr, token_eval, assume_unique=True)\n",
    "                        is_not = False\n",
    "                    valid = np.union1d(valid, token_eval)\n",
    "                    \n",
    "                    is_or = False\n",
    "                    if not valid.shape: return set(), (is_not, is_and, is_or)\n",
    "                elif is_and:\n",
    "                    if is_not:\n",
    "                        if not_first:\n",
    "                            valid = np.setdiff1d(token_eval, valid, assume_unique=True)\n",
    "                        else:\n",
    "                            valid = np.setdiff1d(valid, token_eval, assume_unique=True)\n",
    "                        is_not = False\n",
    "                    else:\n",
    "                        valid = np.intersect1d(valid, token_eval, assume_unique=True)\n",
    "                    \n",
    "                    is_and = False\n",
    "                    if not valid.shape: return set(), (is_not, is_and, is_or)\n",
    "                else:\n",
    "                    valid = token_eval\n",
    "            else:\n",
    "                # unseen word or stopword\n",
    "                is_not = is_or = False\n",
    "    if isinstance(valid, np.ndarray):\n",
    "        valid = set(valid.tolist())\n",
    "    return valid, (is_not, is_and, is_or)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bool_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(bool_search(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine AND (okay AND good) AND happy\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bool_search' is not defined"
     ]
    }
   ],
   "source": [
    "len(bool_search(\"fine AND (okay AND good) AND happy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False, False)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = b = c = False\n",
    "a |= True\n",
    "a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fine AND ', 'okay AND good', ' OR', ' gutenberg']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r\"\\((.+)\\)( +(?:NOT|AND|OR))?\", \"fine AND (okay AND good) OR gutenberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.setdiff1d([1,2,3], [1], assume_unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x72533 sparse matrix of type '<class 'numpy.bool_'>'\n",
       "\twith 973 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_table[2, :] - lookup_table[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last = lookup_table[1, :].indices\n",
    "lookup_table[2, last].indices.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_table[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for &: 'csr_matrix' and 'csr_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lookup_table[\u001b[38;5;241m2\u001b[39m, :] \u001b[38;5;241m&\u001b[39m (lookup_table[\u001b[38;5;241m1\u001b[39m, :])\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for &: 'csr_matrix' and 'csr_matrix'"
     ]
    }
   ],
   "source": [
    "lookup_table[2, :] & (~lookup_table[1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 1, 0, 0], dtype=np.bool_)\n",
    "y = np.array([1, 0, 1, 0], dtype=np.bool_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True, False])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x ^ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, False])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x & (~y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '(3) + 4', '']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_bracket.split(\"((3) + 4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'NOT', ' 3 ', 'OR', ' 4 ', 'AND', ' 5']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_bool_op.split(\"NOT 3 OR 4 AND 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_table: scipy.sparse.csc_matrix = scipy.sparse.load_npz(\"lookup_table.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_arr = scipy.sparse.csc_array(lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = table_arr[[1], :] == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[0]].ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x196 sparse array of type '<class 'numpy.bool_'>'\n",
       "\twith 22 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_arr[2, np.nonzero(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x2 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 2 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "scipy.sparse.csc_matrix([[0, 1], [2, 3]])[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x2 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 2 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.sparse.csc_matrix([[0, 1], [2, 3]])[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_size = 5000\n",
    "part_size = 5000\n",
    "\n",
    "# for i in range(13):\n",
    "#     gc.collect()\n",
    "#     print(f\"\\nPart {i:02d}\", flush=True)\n",
    "#     indexing.build_full_index(offset=i*part_size, k=part_size, batch_size=segment_size, index_type=\"inverted\", prefix=f\"part{i:02d}\", unsafe_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sizes = utils.measure_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(sizes.items())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing.merge_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140 segments to load\n",
      "Finished merging 1140 segments...\n",
      "Garbage collection done\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "index = loader.load_merged_index(save_merged=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44GB 370MB\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "from pympler.asizeof import asizeof\n",
    "index_size = asizeof(index)\n",
    "mb = 1024 * 1024\n",
    "gb = mb * 1024\n",
    "gb_size = index_size // gb\n",
    "mb_size = round((index_size % gb) / mb)\n",
    "print(f\"{gb_size}GB\" + f\" {mb_size}MB\" if mb_size else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = loader.load_token_vocab(k=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"valid_books.pkl\", \"rb\") as f:\n",
    "    _, _, valid_books = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5696023, 5696023)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tokens), len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    USE_TQDM = True\n",
    "except:\n",
    "    USE_TQDM = False\n",
    "\n",
    "def construct_bool_table(index: Iterable[dict], save_path: str|None=None):\n",
    "    table = scipy.sparse.dok_matrix((len(all_tokens), max(valid_books) + 1), dtype=np.bool_)\n",
    "    print(len(index), table.shape)\n",
    "    length = len(all_tokens)\n",
    "    tqdm_iter = enumerate(index[:length])\n",
    "    if USE_TQDM:\n",
    "        tqdm_iter = tqdm(tqdm_iter, total=length)\n",
    "    for token_id, token_dict in tqdm_iter:\n",
    "        if token_dict:\n",
    "            table[token_id, tuple(token_dict.keys())] = True\n",
    "    gc.collect()\n",
    "    table = table.tocsr()\n",
    "    if save_path is None:\n",
    "        return table\n",
    "    else:\n",
    "        gc.collect()\n",
    "        scipy.sparse.save_npz(\"lookup_table.npz\", table, compressed=True)\n",
    "        return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5696023 (5696023, 72533)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc48bab05af4c6f93cf6568a7976fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5696023 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = construct_bool_table(index)\n",
    "scipy.sparse.save_npz(\"lookup_table.npz\", table, compressed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1063"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(table.data.nbytes + table.indptr.nbytes + table.indices.nbytes) // 1024 // 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dok_table: scipy.sparse.dok_matrix = table.todok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asizeof(dok_table) // 1024 // 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asizeof(table) // 1024 // 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz(\"lookup_table_coo.npz\", table.tocoo(), compressed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 5695000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = list(range(5696023))\n",
    "y = []\n",
    "index_size = len(x)\n",
    "batch_size = 5000\n",
    "batches = index_size // batch_size\n",
    "for i in range(batches):\n",
    "    start = i * batch_size\n",
    "    end = min(start + batch_size, index_size)\n",
    "    y.extend(x[start:end])\n",
    "\n",
    "y.extend(x[end:])\n",
    "x == y, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5696023"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"all_tokens_old.pkl\", 'rb') as f:\n",
    "    _, _, all_tokens_old = pickle.load(f)\n",
    "len(all_tokens_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverted_index_2, indexed_books_2 = indexing.build_full_index(offset=5000, k=5000, batch_size=-1, index_type=\"inverted\", prefix=\"part02\")\n",
    "# import h5py\n",
    "# def save_inv_index_HDF5(filename: str, index: list[dict], **kwargs):\n",
    "#     with h5py.File(filename, 'w') as f:\n",
    "#         for i, entry in enumerate(index):\n",
    "#             group = f.create_group(str(i))\n",
    "#             for book_id, occurrences in entry.items():\n",
    "#                 group.create_dataset(str(book_id), data=occurrences, **kwargs)\n",
    "# save_inv_index_HDF5(\"test.h5\", inverted_index_1, chunks=True, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(\"index/inverted_113.pkl\", \"rb\") as f:\n",
    "    tmp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bow_index, indexed_books = indexing.build_full_index(k=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(\"index.pkl\", \"rb\") as f:\n",
    "    bow_index = pickle.load(f)\n",
    "# dummy_arr = np.array([], dtype=np.uint8)\n",
    "# dummy = (0, dummy_arr, 0, dummy_arr)\n",
    "# bow_index = tuple([dummy] + [pair if isinstance(pair, tuple) else dummy for pair in bow_index[1:]])\n",
    "# with open(\"index.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(bow_index, f)\n",
    "\n",
    "with open(\"valid_books.pkl\", \"rb\") as f:\n",
    "    _, _, valid_books = pickle.load(f)\n",
    "\n",
    "book_list = sorted(valid_books)\n",
    "\n",
    "non_empty_books = []\n",
    "non_empty_index = []\n",
    "for book_id, book_bow in enumerate(bow_index):\n",
    "    if book_bow:\n",
    "        non_empty_books.append(book_id)\n",
    "        non_empty_index.append(book_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import MatrixSimilarity, SparseMatrixSimilarity\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [utils.deltazip([vocab, counts], [vocab_delta, count_delta]) for vocab_delta, vocab, count_delta, counts in non_empty_index]\n",
    "# tfidf_model = TfidfModel(corpus, smartirs='ntc')\n",
    "# with open(\"tfidf_model.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(tfidf_model, f)\n",
    "with open(\"tfidf_model.pkl\", \"rb\") as f:\n",
    "    tfidf_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(all_tokens) + 1\n",
    "def cast2numpy(x):\n",
    "    arr = np.zeros(dim, dtype=np.float16)\n",
    "    for i, score in x:\n",
    "        arr[i] = score\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [tfidf_model[utils.deltazip([vocab, counts], [vocab_delta, count_delta])] for vocab_delta, vocab, count_delta, counts in non_empty_index]\n",
    "# with open(\"tfidf_index.pkl\", \"rb\") as f:\n",
    "#     tmp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model[[(0,1),(3,8)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_delta, vocab, count_delta, counts = non_empty_index[0]\n",
    "cast2numpy(tfidf_model[utils.deltazip([vocab, counts], [vocab_delta, count_delta])]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(all_tokens) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = SparseMatrixSimilarity(tmp, num_best=10, num_features=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tfidf_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tmp, f)\n",
    "with open(\"matrix.pkl\", \"wb\") as f:\n",
    "    pickle.dump(m1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asizeof.asizeof(tmp)/1024/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asizeof.asizeof(m1)/1024/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_delta, vocab, count_delta, counts = non_empty_index[269]\n",
    "next(utils.deltazip([vocab, counts], [vocab_delta, count_delta]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty_index[269]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [1, 3, 1, 9, 3, 5654]\n",
    "vocab_list = sorted(set(tokens))\n",
    "token_arr = np.array(tokens)\n",
    "vocab, vocab_delta = utils.cast2intarr(vocab_list)\n",
    "counts, count_delta = utils.cast2intarr([np.sum(token_arr == token) for token in vocab_list])\n",
    "(vocab_delta, vocab, count_delta, counts, vocab_list, [(vocab == token-vocab_delta) for token in vocab_list])\n",
    "vocab + vocab_delta, counts + count_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_path_template = loader.token_dir + \"PG%d_tokens.txt\"\n",
    "\n",
    "def read_tokens(PG_id: int):\n",
    "    with open(book_path_template %PG_id, encoding=\"UTF-8\", errors=\"ignore\") as f:\n",
    "        return loader.stemmer.stemWords(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "docs = []\n",
    "docs_index = []\n",
    "failed_jobs = 0\n",
    "complete_counter = 0\n",
    "with concurrent.futures.ThreadPoolExecutor() as pool:\n",
    "    jobs = {\n",
    "        pool.submit(\n",
    "            read_tokens, book_id)\n",
    "            : book_id for book_id in indexed_books\n",
    "        }\n",
    "    \n",
    "    for job in concurrent.futures.as_completed(jobs):\n",
    "        book_id = jobs[job]\n",
    "        try:\n",
    "            result = job.result()\n",
    "            docs.append(result)\n",
    "            docs_index.append(book_id)\n",
    "        except Exception as e:\n",
    "            # raise e\n",
    "            failed_jobs.append(book_id)\n",
    "        complete_counter += 1\n",
    "        print(f\"Finished fetching tokens in {complete_counter} books...\", end=\"\\r\")\n",
    "        \n",
    "        print(f\"\\n{len(failed_jobs)}/{len(jobs)} token fetching jobs failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "del docs\n",
    "tfidf = TfidfModel(corpus, smartirs='ntc')\n",
    "tfidf_corpus = [tfidf[doc] for doc in corpus]\n",
    "del corpus, tfidf\n",
    "index = MatrixSimilarity(tfidf_corpus, num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"all_tokens.pkl\", 'rb') as f:\n",
    "#     k, offset, _all_tokens = pickle.load(f)\n",
    "# with open(\"all_tokens.pkl\", \"wb\") as f:\n",
    "#     pickle.dump((k, offset, tuple(_all_tokens)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import asizeof\n",
    "asizeof.asizeof(bow_index), asizeof.asizeof(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "415963552/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "415971552/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mem_size_lst = asizeof.asizeof(all_tokens)\n",
    "mem_size_tup = asizeof.asizeof(tuple(all_tokens))\n",
    "mem_size_lst/1024/1024, mem_size_tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "import numpy as np\n",
    "log2(len(all_tokens))\n",
    "x = np.array([1,2,3])\n",
    "x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader.build_full_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"index.pkl\", \"rb\") as f:\n",
    "    index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import asizeof\n",
    "mem_size = asizeof.asizeof(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_size/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0\n",
    "print(len(index))\n",
    "for i, token_eval in enumerate(index):\n",
    "    for arr in token_eval.values():\n",
    "        size += asizeof.asizeof(arr)\n",
    "    print(i, end='\\r')\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size/1024/1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
